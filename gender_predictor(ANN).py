# -*- coding: utf-8 -*-
"""Gender Predictor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14hMYY8v5KM9MnYhVTSIuLxhhYg4ESCpM
"""

import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pickle

# Part 1 - Data Preprocessing
# Importing the dataset
from google.colab import files
uploaded = files.upload()

import io
data = pd.read_csv(io.BytesIO(uploaded['weight-height.csv']))
data.head()

X=data.iloc[:,[1,2]]
y=data.iloc[:,0].values
from scipy import stats
z = np.abs(stats.zscore(X))
print(z)
threshold = 3
print(np.where(z > 3))
new_data = data[(z < 3).all(axis=1)]
new_data

X=new_data.iloc[:,[1,2]]

height=X["Height"]/12
weight=X['Weight']*0.453592

df = pd.DataFrame(height) 
df2 = pd.DataFrame(weight) 
#X = pd.concat(df,df2)

frames=[df,df2]
X = pd.concat(frames,axis=1)

X.values
y=new_data.iloc[:,0].values

from sklearn.preprocessing import LabelEncoder  # Encoding library call
enc = LabelEncoder()
label_encoder = enc.fit(y)
label_encoder
Y = label_encoder.transform(y)
Y

## Spliting your data into training set and test set
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.1,random_state=42)

# Importing the Keras libraries and packages
import keras
from keras.models import Sequential     #responsible for creating any kid of NN like ANN, CNN, RNN
from keras.layers import Dense          #for creating hidden layers
#from keras.layers import LeakyReLU,PReLU,ELU      #Diffrent types of activation functions
from keras.layers import Dropout        #A regularization parameter used for regularization of dataset

# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 12, kernel_initializer = 'he_normal',activation='relu',input_dim = 2))    #output_dim means the number of neurons in 1st hidden layer output dimention got updated as units
                                                                                                        #init means the weight initialization techniques that we are using but init is got updated as kernel_initializer
                                                                                                        #activation means the activatio function that we are using
                                                                                                        #input_dim means how many inupt features are connected to the hidden layer

# Adding the second hidden layer
classifier.add(Dense(units = 12, kernel_initializer = 'he_normal',activation='relu'))

classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])   #optimizer Adam is best for compiling
                                                                                               #Loss is binary_crossentropy beacause we are using categorical output as 0 and 1 if a regression problem we should use categorical_crossentropy

classifier.summary()

# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, Y_train,validation_split=0.33, batch_size = 10, nb_epoch = 70)      #Advantages of using a batch size < number of all samples: It requires less memory. Since you train the network using fewer samples, the overall training procedure requires less memory. and uses less time and less space in Ram
                                                                                                           #nb_epoch is number of epoch its gonna tak while training
